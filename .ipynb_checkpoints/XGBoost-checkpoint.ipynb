{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme gradient boosting from scratch\n",
    "\n",
    "(not finished yet..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "from DecisionTree import DecisionTree\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "class LogisticLoss():\n",
    "    def __init__(self):\n",
    "        sigmoid = Sigmoid()\n",
    "        self.log_func = sigmoid\n",
    "        self.log_grad = sigmoid.gradient\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        p = self.log_func(y_pred)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    # gradient w.r.t y_pred\n",
    "    def gradient(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return -(y - p)\n",
    "\n",
    "    # w.r.t y_pred\n",
    "    def hess(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return p * (1 - p)\n",
    "\n",
    "class XGBoostRegressionTree(DecisionTree):\n",
    "    def __init__(self, loss=None):\n",
    "        self.loss = LogisticLoss()\n",
    "    \n",
    "    def _split(self, y):\n",
    "        col = int(np.shape(y)[1]/2)\n",
    "        y, y_pred = y[:, :col], y[:, col:]\n",
    "        return y, y_pred\n",
    "\n",
    "    def _gain(self, y, y_pred):\n",
    "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (nominator / denominator)\n",
    "\n",
    "    def _gain_by_taylor(self, y, y1, y2):\n",
    "        y, y_pred = self._split(y)\n",
    "        y1, y1_pred = self._split(y1)\n",
    "        y2, y2_pred = self._split(y2)\n",
    "\n",
    "        true_gain = self._gain(y1, y1_pred)\n",
    "        false_gain = self._gain(y2, y2_pred)\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "\n",
    "    def _approximate_update(self, y):\n",
    "        y, y_pred = self._split(y)\n",
    "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)\n",
    "        update_approximation =  gradient / hessian\n",
    "        return update_approximation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._gain_by_taylor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super(XGBoostRegressionTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,) (40,) (40,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   8.80922888,  12.        ],\n",
       "       [  1.        ,  10.73808706,  12.        ],\n",
       "       [  2.        ,   8.37236881,  12.        ],\n",
       "       [  3.        ,  10.2762582 ,  12.        ],\n",
       "       [  4.        ,  11.86385513,  12.        ],\n",
       "       [  5.        ,  10.2936596 ,  12.        ],\n",
       "       [  6.        ,  11.21639795,  12.        ],\n",
       "       [  7.        ,   9.24619167,  12.        ],\n",
       "       [  8.        ,  11.8243575 ,  12.        ],\n",
       "       [  9.        ,  11.09790609,  12.        ],\n",
       "       [ 10.        ,  20.10270769,  22.        ],\n",
       "       [ 11.        ,  20.50609911,  22.        ],\n",
       "       [ 12.        ,  19.62522847,  22.        ],\n",
       "       [ 13.        ,  20.16336732,  22.        ],\n",
       "       [ 14.        ,  21.46668298,  22.        ],\n",
       "       [ 15.        ,  21.54202491,  22.        ],\n",
       "       [ 16.        ,  18.87245436,  22.        ],\n",
       "       [ 17.        ,  19.01306564,  22.        ],\n",
       "       [ 18.        ,  19.54553673,  22.        ],\n",
       "       [ 19.        ,  19.72121023,  22.        ],\n",
       "       [ 20.        ,   0.73971648,   2.        ],\n",
       "       [ 21.        ,   0.20007347,   2.        ],\n",
       "       [ 22.        ,  -1.34176995,   2.        ],\n",
       "       [ 23.        ,  -1.51435736,   2.        ],\n",
       "       [ 24.        ,  -0.74534242,   2.        ],\n",
       "       [ 25.        ,   1.83277013,   2.        ],\n",
       "       [ 26.        ,   0.50033333,   2.        ],\n",
       "       [ 27.        ,  -0.78758112,   2.        ],\n",
       "       [ 28.        ,   0.82419177,   2.        ],\n",
       "       [ 29.        ,   0.84732772,   2.        ],\n",
       "       [ 30.        ,  29.08443578,  30.        ],\n",
       "       [ 31.        ,  29.91070816,  30.        ],\n",
       "       [ 32.        ,  30.8059917 ,  30.        ],\n",
       "       [ 33.        ,  29.25468453,  30.        ],\n",
       "       [ 34.        ,  31.81616101,  30.        ],\n",
       "       [ 35.        ,  28.78132597,  30.        ],\n",
       "       [ 36.        ,  31.77269032,  30.        ],\n",
       "       [ 37.        ,  30.61458111,  30.        ],\n",
       "       [ 38.        ,  31.33814211,  30.        ],\n",
       "       [ 39.        ,  28.10313509,  30.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBoostRegressionTree()\n",
    "\n",
    "X = np.arange(0,40)\n",
    "\n",
    "y1 = np.random.uniform(8,12,10)\n",
    "y2 = np.random.uniform(18,22,10)\n",
    "y3 = np.random.uniform(-2,2,10)\n",
    "y4 = np.random.uniform(28,32,10)\n",
    "\n",
    "y = np.concatenate((y1,y2,y3,y4))\n",
    "y_pred = np.array([12]*10 + [22] * 10 + [2] * 10 + [30] * 10)\n",
    "\n",
    "print(X.shape, y.shape, y_pred.shape)\n",
    "data = np.vstack((X, y, y_pred)).T\n",
    "data\n",
    "xgb._split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigmoid().gradient(0)\n",
    "\n",
    "#ll = LogisticLoss()\n",
    "#ll.gradient(29, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
