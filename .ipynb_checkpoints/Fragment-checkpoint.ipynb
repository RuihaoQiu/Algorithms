{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Bayes for nlp\n",
    "\n",
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def tokenize(message):\n",
    "    message = message.lower()\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)\n",
    "    return set(all_words)\n",
    "\n",
    "\"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "def count_words(training_set):\n",
    "    counts = collections.defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts\n",
    "\n",
    "\"\"\"turn the word_counts into a list of triplets - w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    return [(w, (spam + k) / (total_spams + 2 * k), (non_spam + k) / (total_non_spams + 2 * k)) \n",
    "            for w, (spam, non_spam) in counts.items()]\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "                \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "    def train(self, training_set):\n",
    "        num_spams = len([is_spam\n",
    "                            for message, is_spam in training_set\n",
    "                            if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        \n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                                num_spams,\n",
    "                                                num_non_spams,\n",
    "                                                self.k)\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "authors_file = \"data/email_authors.pkl\"\n",
    "authors_file_handler = open(authors_file, \"rb\")\n",
    "authors = pickle.load(authors_file_handler)\n",
    "authors_file_handler.close()\n",
    "\n",
    "words_file = \"data/word_data.pkl\"\n",
    "words_file_handler = open(words_file, \"rb\")\n",
    "word_data = pickle.load(words_file_handler)\n",
    "words_file_handler.close()\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "train_data = list(zip(features_train, labels_train))\n",
    "test_data = list(zip(features_test, labels_test))\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_data[:100])\n",
    "\n",
    "classified = [(subject, is_spam, classifier.classify(subject)) for subject, is_spam in test_data[:100]]\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "text = features_train[:30]\n",
    "\n",
    "def vector_counts(text):\n",
    "    n_doc = len(text)\n",
    "    counts = collections.defaultdict(lambda: [0]*n_doc)\n",
    "    for n,m in enumerate(text):\n",
    "        for word in tokenize(m):\n",
    "                counts[word][n] += 1\n",
    "    mx = np.array([k for k in counts.values()]).T\n",
    "    return mx\n",
    "\n",
    "def Tfidf_transformer(vector):\n",
    "    n_d = vector.shape[0]\n",
    "    df_t = np.count_nonzero(vector, axis=0)\n",
    "    idf_t = np.log(n_d/df_t) + 1\n",
    "    tf_idf = vector * idf_t[None,:]\n",
    "    tf_idf = np.divide(tf_idf, LA.norm(tf_idf, axis=1)[:,None])\n",
    "    return tf_idf\n",
    "    \n",
    "Tfidf_transformer(vector_counts(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## from decision tree\n",
    "# coding: utf-8\n",
    "\n",
    "# In[164]:\n",
    "\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# In[123]:\n",
    "\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "training_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 3, 'Apple'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon']\n",
    "]\n",
    "\n",
    "pd.DataFrame(training_data, columns=header)\n",
    "\n",
    "\n",
    "# In[124]:\n",
    "\n",
    "def is_numeric(value):\n",
    "    return isinstance(value, int) or isinstance(value, float)\n",
    "\n",
    "\n",
    "# In[125]:\n",
    "\n",
    "## Ask question\n",
    "class Question:\n",
    "    def __init__(self, column, value):\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "\n",
    "    def match(self, example):\n",
    "        val = example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val >= self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "\n",
    "    def __repr__(self):\n",
    "        condition = \"==\"\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s %s %s?\" %(header[self.column], condition, str(self.value))\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "Question(0, 'red'), Question(1, 2)\n",
    "\n",
    "\n",
    "# In[127]:\n",
    "\n",
    "## Partition a dataset.\n",
    "def partition(rows, question):\n",
    "    true_rows, false_rows = [], []\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "    return true_rows, false_rows\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "q = Question(1, 2)\n",
    "true_rows, false_rows = partition(training_data, q)\n",
    "print(\"True rows: %s\" %true_rows, \"\\nFalse rows: %s\" %false_rows)\n",
    "\n",
    "\n",
    "# In[129]:\n",
    "\n",
    "## Count labels for each class\n",
    "def class_counts(rows):\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "# In[220]:\n",
    "\n",
    "A = class_counts(training_data)\n",
    "print(A)\n",
    "\n",
    "\n",
    "# In[222]:\n",
    "\n",
    "max(set(A), key=A.get)\n",
    "\n",
    "\n",
    "# In[131]:\n",
    "\n",
    "## Gini impurity defined from -- https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity\n",
    "## Gini_impurity = 1 - Sum(p**2)\n",
    "def gini(rows):\n",
    "    counts = class_counts(rows)\n",
    "    impurity = 1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl = counts[lbl] / float(len(rows))\n",
    "        impurity -= prob_of_lbl**2\n",
    "    return impurity\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "gini(training_data)\n",
    "\n",
    "\n",
    "# In[133]:\n",
    "\n",
    "## Information gain is used to decide which feature to split on at each step in building the tree.\n",
    "## Information gain = entropy(parent) - entropy(children)\n",
    "\n",
    "def info_gain(left, right, current_uncertainty):\n",
    "    p = float(len(left)) / (len(left) + len(right))\n",
    "    return current_uncertainty - (p * gini(left) + (1 - p) * gini(right))\n",
    "\n",
    "\n",
    "# In[134]:\n",
    "\n",
    "current_uncertainty = gini(training_data)\n",
    "true_rows, false_rows = partition(training_data, Question(0, 'Green'))\n",
    "info_gain(true_rows, false_rows, current_uncertainty)\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "## Find best split (question) with largest information gain\n",
    "def find_best_split(rows):\n",
    "    best_gain = 0\n",
    "    best_question = None\n",
    "    current_uncertainty = gini(rows)\n",
    "    n_features = len(rows[0]) - 1\n",
    "\n",
    "    for col in range(n_features):\n",
    "        values = set([row[col] for row in rows])\n",
    "        for val in values:\n",
    "            question = Question(col, val)\n",
    "            true_rows, false_rows = partition(rows, question)\n",
    "            if len(true_rows) == 0 or len(false_rows) == 0:\n",
    "                continue\n",
    "            gain = info_gain(true_rows, false_rows, current_uncertainty)\n",
    "            if gain >= best_gain:\n",
    "                best_gain, best_question = gain, question\n",
    "\n",
    "    return best_gain, best_question\n",
    "\n",
    "\n",
    "# In[138]:\n",
    "\n",
    "best_gain, best_question = find_best_split(training_data)\n",
    "best_gain, best_question\n",
    "\n",
    "\n",
    "# In[139]:\n",
    "\n",
    "## Define leaf and node\n",
    "class Leaf:\n",
    "    def __init__(self, rows):\n",
    "        self.predictions = class_counts(rows)\n",
    "        \n",
    "class Decision_Node:\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question = question\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch\n",
    "\n",
    "\n",
    "# In[140]:\n",
    "\n",
    "## Build the tree, split until the information gain = 0\n",
    "## This is the main part\n",
    "def build_tree(rows):\n",
    "    gain, question = find_best_split(rows)\n",
    "    if gain == 0:\n",
    "        return Leaf(rows)\n",
    "    true_rows, false_rows = partition(rows, question)\n",
    "    true_branch = build_tree(true_rows)\n",
    "    false_branch = build_tree(false_rows)\n",
    "    return Decision_Node(question, true_branch, false_branch)\n",
    "\n",
    "\n",
    "# In[141]:\n",
    "\n",
    "## Print the whole tree\n",
    "def print_tree(node, spacing=\"\"):\n",
    "    if isinstance(node, Leaf):\n",
    "        print (spacing + \"Predict\", node.predictions)\n",
    "        return\n",
    "    print (spacing + str(node.question))\n",
    "    print (spacing + '--> True:')\n",
    "    print_tree(node.true_branch, spacing + \"  \")\n",
    "    print (spacing + '--> False:')\n",
    "    print_tree(node.false_branch, spacing + \"  \")\n",
    "\n",
    "\n",
    "# In[156]:\n",
    "\n",
    "my_tree = build_tree(training_data)\n",
    "\n",
    "\n",
    "# In[151]:\n",
    "\n",
    "print_tree(my_tree)\n",
    "\n",
    "\n",
    "# In[157]:\n",
    "\n",
    "def classify(row, node):\n",
    "    if isinstance(node, Leaf):\n",
    "        return node.predictions\n",
    "    if node.question.match(row):\n",
    "        return classify(row, node.true_branch)\n",
    "    else:\n",
    "        return classify(row, node.false_branch)\n",
    "\n",
    "\n",
    "# In[210]:\n",
    "\n",
    "classify(['Green', 3, 'Apple'], my_tree)\n",
    "\n",
    "\n",
    "# In[158]:\n",
    "\n",
    "def print_leaf(counts):\n",
    "    total = sum(counts.values()) * 1.0\n",
    "    probs = {}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl] = str(int(counts[lbl] / total * 100)) + \"%\"\n",
    "    return probs\n",
    "\n",
    "\n",
    "# In[159]:\n",
    "\n",
    "testing_data = [\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Yellow', 4, 'Apple'],\n",
    "    ['Red', 2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon']]\n",
    "\n",
    "\n",
    "# In[160]:\n",
    "\n",
    "for row in testing_data:\n",
    "    print (\"Actual: %s. Predicted: %s\" %\n",
    "           (row[-1], print_leaf(classify(row, my_tree))))\n",
    "\n",
    "\n",
    "# In[244]:\n",
    "\n",
    "## Bagging\n",
    "def subsample(dataset, n_sample):\n",
    "    sample = list()\n",
    "#    n_sample = round(len(dataset) *1.0 / n_samples)\n",
    "    print(n_sample)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "def bagging_classify(row, trees):\n",
    "    c = dict()\n",
    "    [c.update(classify(row, tree)) for tree in trees]\n",
    "    return max(c)\n",
    "#return c\n",
    "\n",
    "\n",
    "# In[245]:\n",
    "\n",
    "def bagging(train, test, sample_size, n_trees):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample)\n",
    "        print_tree(tree)\n",
    "        trees.append(tree)\n",
    "    classes = [bagging_classify(row, trees) for row in test]\n",
    "    return classes\n",
    "\n",
    "\n",
    "# In[249]:\n",
    "\n",
    "bagging(training_data, testing_data, 3, 2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "## Neural network for Scratch\n",
    "## Creat a XOR(“or, but not and”) gate by neural network\n",
    "\n",
    "import math\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def perceptron_output(weights, bias, x):\n",
    "    \"\"\"returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)\n",
    "\n",
    "def sigmoid(t):\n",
    "    return 1 / (1 + math.exp(-t))\n",
    "\n",
    "def neuron_output(weights, inputs):\n",
    "    return sigmoid(dot(weights, inputs))\n",
    "\n",
    "def feed_forward(neural_network, input_vector):\n",
    "    outputs = []\n",
    "       # process one layer at a time\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # add a bias input\n",
    "        output = [neuron_output(neuron, input_with_bias)  # compute the output\n",
    "                     for neuron in layer]                    # for each neuron\n",
    "        outputs.append(output)                            # and remember it\n",
    "           # then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "    return outputs\n",
    "\n",
    "xor_network = [# hidden layer\n",
    "               [[20, 20, -30],\n",
    "                [20, 20, -10]],\n",
    "               # output layer\n",
    "               [[-60, 60, -30]]]\n",
    "for x in [0, 1]:\n",
    "    for y in [0, 1]:\n",
    "        print x, y, feed_forward(xor_network,[x, y])[-1]\n",
    "\n",
    "## Recognize a digit by neural network.\n",
    "\n",
    "import random\n",
    "\n",
    "## backpropagation\n",
    "def backpropagate(network, input_vector, targets):\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "    \n",
    "    # the output * (1 - output) is from the derivative of sigmoid\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                        for output, target in zip(outputs, targets)]\n",
    "    \n",
    "    # adjust weights for output layer, one neuron at a time\n",
    "    for i, output_neuron in enumerate(network[-1]):\n",
    "        for j, hidden_output in enumerate(hidden_outputs + [1]):\n",
    "            output_neuron[j] -= output_deltas[i] * hidden_output\n",
    "\n",
    "    # back-propagate errors to hidden layer        \n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) * \n",
    "                     dot(output_deltas, [n[i] for n in output_layer])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "    \n",
    "    # adjust weights for hidden layer, one neuron at a time\n",
    "    for i, hidden_neuron in enumerate(network[0]):\n",
    "        for j, input in enumerate(input_vector + [1]):\n",
    "            hidden_neuron[j] -= hidden_deltas[i] * input\n",
    "            \n",
    "            \n",
    "random.seed(0)\n",
    "input_size = 25\n",
    "num_hidden = 5\n",
    "output_size = 10\n",
    "\n",
    "inputs =  [[1,1,1,1,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1], \n",
    "           \n",
    "           [0,0,1,0,0,  \n",
    "            0,0,1,0,0,  \n",
    "            0,0,1,0,0,  \n",
    "            0,0,1,0,0,  \n",
    "            0,0,1,0,0], \n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            1,1,1,1,1,  \n",
    "            1,0,0,0,0,  \n",
    "            1,1,1,1,1], \n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            1,1,1,1,1], \n",
    "           \n",
    "           [1,0,0,0,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            0,0,0,0,1], \n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            1,0,0,0,0,  \n",
    "            1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            1,1,1,1,1],\n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            1,0,0,0,0,  \n",
    "            1,1,1,1,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1], \n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            0,0,0,0,1,  \n",
    "            0,0,0,0,1,  \n",
    "            0,0,0,0,1],\n",
    "           \n",
    "           [1,1,1,1,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1],\n",
    "\n",
    "           [1,1,1,1,1,  \n",
    "            1,0,0,0,1,  \n",
    "            1,1,1,1,1,  \n",
    "            0,0,0,0,1,  \n",
    "            1,1,1,1,1]]\n",
    "\n",
    "#targets = 3\n",
    "\n",
    "targets = [[1 if i == j else 0 for i in range(10)]\n",
    "              for j in range(10)]\n",
    "\n",
    "hidden_layer = [[random.random() for _ in range(input_size + 1)]\n",
    "                for _ in range(num_hidden)]\n",
    "\n",
    "output_layer = [[random.random() for _ in range(num_hidden + 1)]\n",
    "                for _ in range(output_size)]\n",
    "\n",
    "network = [hidden_layer, output_layer]\n",
    "\n",
    "for _ in range(10000):\n",
    "    for input_vector, target_vector in zip(inputs, targets):\n",
    "        backpropagate(network, input_vector, target_vector)\n",
    "#    backpropagate(network, inputs, targets)\n",
    "        \n",
    "def predict(input):\n",
    "    return feed_forward(network, input)[-1]\n",
    "\n",
    "print predict(inputs[2])\n",
    "\n",
    "Ref. Data Science from Scratch, Joel Grus"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
