{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "\n",
    "from DecisionTree import DecisionTree\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "class LogisticLoss():\n",
    "    def __init__(self):\n",
    "        sigmoid = Sigmoid()\n",
    "        self.log_func = sigmoid\n",
    "        self.log_grad = sigmoid.gradient\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        p = self.log_func(y_pred)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    # gradient w.r.t y_pred\n",
    "    def gradient(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return -(y - p)\n",
    "\n",
    "    # w.r.t y_pred\n",
    "    def hess(self, y, y_pred):\n",
    "        p = self.log_func(y_pred)\n",
    "        return p * (1 - p)\n",
    "\n",
    "class XGBoostRegressionTree(DecisionTree):\n",
    "    \"\"\"\n",
    "    Regression tree for XGBoost\n",
    "    - Reference -\n",
    "    http://xgboost.readthedocs.io/en/latest/model.html\n",
    "    \"\"\"\n",
    "    def __init__(self, loss=None):\n",
    "        self.loss = LogisticLoss()\n",
    "        print(self.loss)\n",
    "    \n",
    "    def _split(self, y):\n",
    "        \"\"\" y contains y_true in left half of the middle column and\n",
    "        y_pred in the right half. Split and return the two matrices \"\"\"\n",
    "        col = int(np.shape(y)[1]/2)\n",
    "        y, y_pred = y[:, :col], y[:, col:]\n",
    "        return y, y_pred\n",
    "\n",
    "    def _gain(self, y, y_pred):\n",
    "        nominator = np.power((y * self.loss.gradient(y, y_pred)).sum(), 2)\n",
    "        denominator = self.loss.hess(y, y_pred).sum()\n",
    "        return 0.5 * (nominator / denominator)\n",
    "\n",
    "    def _gain_by_taylor(self, y, y1, y2):\n",
    "        # Split\n",
    "        y, y_pred = self._split(y)\n",
    "        y1, y1_pred = self._split(y1)\n",
    "        y2, y2_pred = self._split(y2)\n",
    "\n",
    "        true_gain = self._gain(y1, y1_pred)\n",
    "        false_gain = self._gain(y2, y2_pred)\n",
    "        gain = self._gain(y, y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "\n",
    "    def _approximate_update(self, y):\n",
    "        # y split into y, y_pred\n",
    "        y, y_pred = self._split(y)\n",
    "        # Newton's Method\n",
    "        gradient = np.sum(y * self.loss.gradient(y, y_pred), axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)\n",
    "        update_approximation =  gradient / hessian\n",
    "\n",
    "        return update_approximation\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._gain_by_taylor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super(XGBoostRegressionTree, self).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.LogisticLoss object at 0x1130439b0>\n",
      "(40,) (40,) (40,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,  10.64361619,  12.        ],\n",
       "       [  1.        ,  10.47760546,  12.        ],\n",
       "       [  2.        ,   9.42820294,  12.        ],\n",
       "       [  3.        ,   8.50720965,  12.        ],\n",
       "       [  4.        ,   9.65983814,  12.        ],\n",
       "       [  5.        ,  11.04626485,  12.        ],\n",
       "       [  6.        ,  11.82185969,  12.        ],\n",
       "       [  7.        ,  11.45729804,  12.        ],\n",
       "       [  8.        ,   9.17667352,  12.        ],\n",
       "       [  9.        ,  11.51206648,  12.        ],\n",
       "       [ 10.        ,  19.84649615,  22.        ],\n",
       "       [ 11.        ,  18.90387226,  22.        ],\n",
       "       [ 12.        ,  19.21041789,  22.        ],\n",
       "       [ 13.        ,  21.2474161 ,  22.        ],\n",
       "       [ 14.        ,  18.78540827,  22.        ],\n",
       "       [ 15.        ,  20.66277091,  22.        ],\n",
       "       [ 16.        ,  18.20598042,  22.        ],\n",
       "       [ 17.        ,  20.80195403,  22.        ],\n",
       "       [ 18.        ,  20.12126088,  22.        ],\n",
       "       [ 19.        ,  21.22895349,  22.        ],\n",
       "       [ 20.        ,  -1.26738023,   2.        ],\n",
       "       [ 21.        ,   1.34295998,   2.        ],\n",
       "       [ 22.        ,   1.89338603,   2.        ],\n",
       "       [ 23.        ,   1.44901762,   2.        ],\n",
       "       [ 24.        ,   0.61546773,   2.        ],\n",
       "       [ 25.        ,  -0.86044701,   2.        ],\n",
       "       [ 26.        ,   0.97717351,   2.        ],\n",
       "       [ 27.        ,   0.37413875,   2.        ],\n",
       "       [ 28.        ,  -1.91142621,   2.        ],\n",
       "       [ 29.        ,   0.28317883,   2.        ],\n",
       "       [ 30.        ,  29.36965925,  30.        ],\n",
       "       [ 31.        ,  28.57839931,  30.        ],\n",
       "       [ 32.        ,  28.03533023,  30.        ],\n",
       "       [ 33.        ,  28.81934359,  30.        ],\n",
       "       [ 34.        ,  29.99240441,  30.        ],\n",
       "       [ 35.        ,  29.39353593,  30.        ],\n",
       "       [ 36.        ,  29.14168217,  30.        ],\n",
       "       [ 37.        ,  28.20404712,  30.        ],\n",
       "       [ 38.        ,  30.44973007,  30.        ],\n",
       "       [ 39.        ,  28.06652961,  30.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBoostRegressionTree()\n",
    "\n",
    "X = np.arange(0,40)\n",
    "\n",
    "# just random uniform distributions in differnt range\n",
    "\n",
    "y1 = np.random.uniform(8,12,10)\n",
    "y2 = np.random.uniform(18,22,10)\n",
    "y3 = np.random.uniform(-2,2,10)\n",
    "y4 = np.random.uniform(28,32,10)\n",
    "\n",
    "y = np.concatenate((y1,y2,y3,y4))\n",
    "y_pred = np.array([12]*10 + [22] * 10 + [2] * 10 + [30] * 10)\n",
    "\n",
    "print(X.shape, y.shape, y_pred.shape)\n",
    "data = np.vstack((X, y, y_pred)).T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sigmoid().gradient(0)\n",
    "\n",
    "#ll = LogisticLoss()\n",
    "#ll.gradient(29, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
